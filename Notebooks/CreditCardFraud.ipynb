{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit (conda)",
   "display_name": "Python 3.7.7 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "f5c9dfabb21bd2a91b63810df81acbdc6b5e617e45414f0ef050ca96090c868f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/creditcard.csv', header=\"infer\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Time            V1            V2            V3            V4  \\\n",
       "Time    1.000000  1.173963e-01 -1.059333e-02 -4.196182e-01 -1.052602e-01   \n",
       "V1      0.117396  1.000000e+00  4.697350e-17 -1.424390e-15  1.755316e-17   \n",
       "V2     -0.010593  4.697350e-17  1.000000e+00  2.512175e-16 -1.126388e-16   \n",
       "V3     -0.419618 -1.424390e-15  2.512175e-16  1.000000e+00 -3.416910e-16   \n",
       "V4     -0.105260  1.755316e-17 -1.126388e-16 -3.416910e-16  1.000000e+00   \n",
       "V5      0.173072  6.391162e-17 -2.039868e-16 -1.436514e-15 -1.940929e-15   \n",
       "V6     -0.063016  2.398071e-16  5.024680e-16  1.431581e-15 -2.712659e-16   \n",
       "V7      0.084714  1.991550e-15  3.966486e-16  2.168574e-15  1.556330e-16   \n",
       "V8     -0.036949 -9.490675e-17 -4.413984e-17  3.433113e-16  5.195643e-16   \n",
       "V9     -0.008660  2.169581e-16 -5.728718e-17 -4.233770e-16  3.859585e-16   \n",
       "V10     0.030617  7.433820e-17 -4.782388e-16  6.289267e-16  6.055490e-16   \n",
       "V11    -0.247689  2.438580e-16  9.468995e-16 -5.501758e-17 -2.083600e-16   \n",
       "V12     0.124348  2.422086e-16 -6.588252e-16  2.206522e-16 -5.657963e-16   \n",
       "V13    -0.065902 -2.115458e-16  3.854521e-16 -6.883375e-16 -1.506129e-16   \n",
       "V14    -0.098757  9.352582e-16 -2.541036e-16  4.271336e-16 -8.522435e-17   \n",
       "V15    -0.183453 -3.252451e-16  2.831060e-16  1.122756e-16 -1.507718e-16   \n",
       "V16     0.011903  6.308789e-16  4.934097e-17  1.183364e-15 -6.939204e-16   \n",
       "V17    -0.073297 -5.011524e-16 -9.883008e-16  4.576619e-17 -4.397925e-16   \n",
       "V18     0.090438  2.870125e-16  2.636654e-16  5.427965e-16  1.493667e-16   \n",
       "V19     0.028975  1.818128e-16  9.528280e-17  2.576773e-16 -2.656938e-16   \n",
       "V20    -0.050866  1.036959e-16 -9.309954e-16 -9.429297e-16 -3.223123e-16   \n",
       "V21     0.044736 -1.755072e-16  8.444409e-17 -2.971969e-17 -9.976950e-17   \n",
       "V22     0.144059  7.477367e-17  2.500830e-16  4.648259e-16  2.099922e-16   \n",
       "V23     0.051142  9.808705e-16  1.059562e-16  2.115206e-17  6.002528e-17   \n",
       "V24    -0.016182  7.354269e-17 -8.142354e-18 -9.351637e-17  2.229738e-16   \n",
       "V25    -0.233083 -9.805358e-16 -4.261894e-17  4.771164e-16  5.394585e-16   \n",
       "V26    -0.041407 -8.621897e-17  2.601622e-16  6.521501e-16 -6.179751e-16   \n",
       "V27    -0.005135  3.208233e-17 -4.478472e-16  6.239832e-16 -6.403423e-17   \n",
       "V28    -0.009413  9.820892e-16 -3.676415e-16  7.726948e-16 -5.863664e-17   \n",
       "Amount -0.010596 -2.277087e-01 -5.314089e-01 -2.108805e-01  9.873167e-02   \n",
       "Class  -0.012323 -1.013473e-01  9.128865e-02 -1.929608e-01  1.334475e-01   \n",
       "\n",
       "                  V5            V6            V7            V8            V9  \\\n",
       "Time    1.730721e-01 -6.301647e-02  8.471437e-02 -3.694943e-02 -8.660434e-03   \n",
       "V1      6.391162e-17  2.398071e-16  1.991550e-15 -9.490675e-17  2.169581e-16   \n",
       "V2     -2.039868e-16  5.024680e-16  3.966486e-16 -4.413984e-17 -5.728718e-17   \n",
       "V3     -1.436514e-15  1.431581e-15  2.168574e-15  3.433113e-16 -4.233770e-16   \n",
       "V4     -1.940929e-15 -2.712659e-16  1.556330e-16  5.195643e-16  3.859585e-16   \n",
       "V5      1.000000e+00  7.926364e-16 -4.209851e-16  7.589187e-16  4.205206e-16   \n",
       "V6      7.926364e-16  1.000000e+00  1.429426e-16 -1.707421e-16  1.114447e-16   \n",
       "V7     -4.209851e-16  1.429426e-16  1.000000e+00 -8.691834e-17  7.933251e-16   \n",
       "V8      7.589187e-16 -1.707421e-16 -8.691834e-17  1.000000e+00  2.900829e-16   \n",
       "V9      4.205206e-16  1.114447e-16  7.933251e-16  2.900829e-16  1.000000e+00   \n",
       "V10    -6.601716e-16  2.850776e-16  3.043333e-17  9.051847e-17 -2.771761e-16   \n",
       "V11     7.342759e-16  4.865799e-16 -1.084105e-15  1.954747e-16  4.682341e-16   \n",
       "V12     3.761033e-16  2.140589e-16  1.510045e-15 -6.266057e-17 -2.445230e-15   \n",
       "V13    -9.578659e-16 -2.268061e-16 -9.892325e-17 -2.382948e-16 -2.650351e-16   \n",
       "V14    -3.634803e-16  3.452801e-16 -1.729462e-16 -1.131098e-16  2.343317e-16   \n",
       "V15    -5.132620e-16 -6.368111e-18  1.936832e-17  2.021491e-16 -1.588105e-15   \n",
       "V16    -3.517076e-16 -2.477917e-16  2.893672e-16  5.027192e-16 -3.251906e-16   \n",
       "V17     1.425729e-16  3.567582e-16  1.149692e-15 -3.508777e-16  6.535992e-16   \n",
       "V18     1.109525e-15  2.811474e-16 -1.116789e-16 -4.093852e-16  1.203843e-16   \n",
       "V19    -3.138234e-16  2.717167e-16 -2.874017e-16 -5.339821e-16  1.120752e-16   \n",
       "V20     2.076048e-16  1.898638e-16  1.744242e-16 -1.095534e-16 -4.340941e-16   \n",
       "V21    -1.368701e-16 -1.575903e-16  1.938604e-16 -2.412439e-16  4.578389e-17   \n",
       "V22     5.060029e-16 -3.362902e-16 -1.058131e-15  5.475559e-16  2.871855e-17   \n",
       "V23     1.637596e-16 -7.232186e-17  2.327911e-16  3.897104e-16  5.929286e-16   \n",
       "V24    -9.286095e-16 -1.261867e-15 -2.589727e-17 -1.802967e-16 -2.346385e-16   \n",
       "V25     5.625102e-16  1.081933e-15  1.174169e-15 -1.390791e-16  1.099645e-15   \n",
       "V26     9.144690e-16 -2.378414e-16 -7.334507e-16 -1.209975e-16 -1.388725e-15   \n",
       "V27     4.465960e-16 -2.623818e-16 -5.886825e-16  1.733633e-16 -2.287414e-16   \n",
       "V28    -3.299167e-16  4.813155e-16 -6.836764e-17 -4.484325e-16  9.146779e-16   \n",
       "Amount -3.863563e-01  2.159812e-01  3.973113e-01 -1.030791e-01 -4.424560e-02   \n",
       "Class  -9.497430e-02 -4.364316e-02 -1.872566e-01  1.987512e-02 -9.773269e-02   \n",
       "\n",
       "        ...           V21           V22           V23           V24  \\\n",
       "Time    ...  4.473573e-02  1.440591e-01  5.114236e-02 -1.618187e-02   \n",
       "V1      ... -1.755072e-16  7.477367e-17  9.808705e-16  7.354269e-17   \n",
       "V2      ...  8.444409e-17  2.500830e-16  1.059562e-16 -8.142354e-18   \n",
       "V3      ... -2.971969e-17  4.648259e-16  2.115206e-17 -9.351637e-17   \n",
       "V4      ... -9.976950e-17  2.099922e-16  6.002528e-17  2.229738e-16   \n",
       "V5      ... -1.368701e-16  5.060029e-16  1.637596e-16 -9.286095e-16   \n",
       "V6      ... -1.575903e-16 -3.362902e-16 -7.232186e-17 -1.261867e-15   \n",
       "V7      ...  1.938604e-16 -1.058131e-15  2.327911e-16 -2.589727e-17   \n",
       "V8      ... -2.412439e-16  5.475559e-16  3.897104e-16 -1.802967e-16   \n",
       "V9      ...  4.578389e-17  2.871855e-17  5.929286e-16 -2.346385e-16   \n",
       "V10     ...  8.089504e-16 -6.707598e-16  3.809732e-16 -4.032806e-17   \n",
       "V11     ... -3.911893e-16 -3.811640e-17  2.232007e-16  1.219849e-15   \n",
       "V12     ...  3.229576e-16 -5.903992e-16  1.392162e-16  4.901644e-16   \n",
       "V13     ...  9.499130e-17 -2.659710e-17 -5.884304e-16 -5.470547e-16   \n",
       "V14     ...  1.634141e-17  3.439699e-16  7.620728e-17  2.335749e-16   \n",
       "V15     ...  1.947458e-17 -8.936817e-16  1.119827e-16 -4.589689e-16   \n",
       "V16     ... -3.927401e-16  3.878384e-17  8.519670e-16 -4.289239e-16   \n",
       "V17     ... -7.753967e-16 -8.389703e-16  5.367784e-16 -5.543631e-17   \n",
       "V18     ... -1.140973e-15 -8.662635e-17 -3.624236e-16 -1.126043e-16   \n",
       "V19     ...  4.032541e-16 -9.690436e-16  5.733798e-16  3.126716e-17   \n",
       "V20     ... -1.120828e-15  1.105842e-15  4.986739e-16  1.637488e-16   \n",
       "V21     ...  1.000000e+00  3.905948e-15  6.127323e-16  1.298254e-16   \n",
       "V22     ...  3.905948e-15  1.000000e+00  3.130812e-16  1.150829e-17   \n",
       "V23     ...  6.127323e-16  3.130812e-16  1.000000e+00 -4.411271e-17   \n",
       "V24     ...  1.298254e-16  1.150829e-17 -4.411271e-17  1.000000e+00   \n",
       "V25     ... -2.826293e-16 -6.078986e-16 -9.938362e-16  1.557318e-15   \n",
       "V26     ... -4.907301e-16 -8.477050e-16  8.848700e-16  3.129195e-16   \n",
       "V27     ... -1.033403e-15 -1.294910e-16  5.524044e-16 -3.736529e-16   \n",
       "V28     ...  5.132234e-16 -3.021376e-16  9.029821e-16 -2.259275e-16   \n",
       "Amount  ...  1.059989e-01 -6.480065e-02 -1.126326e-01  5.146217e-03   \n",
       "Class   ...  4.041338e-02  8.053175e-04 -2.685156e-03 -7.220907e-03   \n",
       "\n",
       "                 V25           V26           V27           V28    Amount  \\\n",
       "Time   -2.330828e-01 -4.140710e-02 -5.134591e-03 -9.412688e-03 -0.010596   \n",
       "V1     -9.805358e-16 -8.621897e-17  3.208233e-17  9.820892e-16 -0.227709   \n",
       "V2     -4.261894e-17  2.601622e-16 -4.478472e-16 -3.676415e-16 -0.531409   \n",
       "V3      4.771164e-16  6.521501e-16  6.239832e-16  7.726948e-16 -0.210880   \n",
       "V4      5.394585e-16 -6.179751e-16 -6.403423e-17 -5.863664e-17  0.098732   \n",
       "V5      5.625102e-16  9.144690e-16  4.465960e-16 -3.299167e-16 -0.386356   \n",
       "V6      1.081933e-15 -2.378414e-16 -2.623818e-16  4.813155e-16  0.215981   \n",
       "V7      1.174169e-15 -7.334507e-16 -5.886825e-16 -6.836764e-17  0.397311   \n",
       "V8     -1.390791e-16 -1.209975e-16  1.733633e-16 -4.484325e-16 -0.103079   \n",
       "V9      1.099645e-15 -1.388725e-15 -2.287414e-16  9.146779e-16 -0.044246   \n",
       "V10    -2.863813e-16 -2.554293e-16 -3.103239e-16 -1.515934e-16 -0.101502   \n",
       "V11    -4.567635e-16 -1.110976e-16 -2.635827e-16 -3.091914e-16  0.000104   \n",
       "V12     5.053736e-16 -5.759321e-16 -2.312619e-16  7.327446e-16 -0.009542   \n",
       "V13     8.066738e-17 -2.121518e-16 -4.520414e-16  1.049541e-15  0.005293   \n",
       "V14    -2.606783e-16 -6.580254e-18  1.285770e-16  2.503271e-15  0.033751   \n",
       "V15     3.869740e-16  3.761094e-16 -1.265235e-15 -1.063286e-15 -0.002986   \n",
       "V16    -6.644104e-16 -5.186503e-16  7.820038e-16  8.637186e-16 -0.003910   \n",
       "V17     4.822068e-16  4.870302e-16  8.844373e-16 -2.182692e-16  0.007309   \n",
       "V18    -2.310856e-16  3.183964e-16  2.435170e-16  8.844995e-16  0.035650   \n",
       "V19     7.415355e-16  5.614354e-16 -1.113035e-16 -1.375843e-15 -0.056151   \n",
       "V20    -1.518242e-16 -2.975081e-16 -1.446069e-15 -1.133579e-16  0.339403   \n",
       "V21    -2.826293e-16 -4.907301e-16 -1.033403e-15  5.132234e-16  0.105999   \n",
       "V22    -6.078986e-16 -8.477050e-16 -1.294910e-16 -3.021376e-16 -0.064801   \n",
       "V23    -9.938362e-16  8.848700e-16  5.524044e-16  9.029821e-16 -0.112633   \n",
       "V24     1.557318e-15  3.129195e-16 -3.736529e-16 -2.259275e-16  0.005146   \n",
       "V25     1.000000e+00  2.810884e-15 -6.107118e-16  3.399375e-16 -0.047837   \n",
       "V26     2.810884e-15  1.000000e+00 -3.383861e-16 -3.751403e-16 -0.003208   \n",
       "V27    -6.107118e-16 -3.383861e-16  1.000000e+00 -3.770124e-16  0.028825   \n",
       "V28     3.399375e-16 -3.751403e-16 -3.770124e-16  1.000000e+00  0.010258   \n",
       "Amount -4.783686e-02 -3.208037e-03  2.882546e-02  1.025822e-02  1.000000   \n",
       "Class   3.307706e-03  4.455398e-03  1.757973e-02  9.536041e-03  0.005632   \n",
       "\n",
       "           Class  \n",
       "Time   -0.012323  \n",
       "V1     -0.101347  \n",
       "V2      0.091289  \n",
       "V3     -0.192961  \n",
       "V4      0.133447  \n",
       "V5     -0.094974  \n",
       "V6     -0.043643  \n",
       "V7     -0.187257  \n",
       "V8      0.019875  \n",
       "V9     -0.097733  \n",
       "V10    -0.216883  \n",
       "V11     0.154876  \n",
       "V12    -0.260593  \n",
       "V13    -0.004570  \n",
       "V14    -0.302544  \n",
       "V15    -0.004223  \n",
       "V16    -0.196539  \n",
       "V17    -0.326481  \n",
       "V18    -0.111485  \n",
       "V19     0.034783  \n",
       "V20     0.020090  \n",
       "V21     0.040413  \n",
       "V22     0.000805  \n",
       "V23    -0.002685  \n",
       "V24    -0.007221  \n",
       "V25     0.003308  \n",
       "V26     0.004455  \n",
       "V27     0.017580  \n",
       "V28     0.009536  \n",
       "Amount  0.005632  \n",
       "Class   1.000000  \n",
       "\n",
       "[31 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Time</th>\n      <td>1.000000</td>\n      <td>1.173963e-01</td>\n      <td>-1.059333e-02</td>\n      <td>-4.196182e-01</td>\n      <td>-1.052602e-01</td>\n      <td>1.730721e-01</td>\n      <td>-6.301647e-02</td>\n      <td>8.471437e-02</td>\n      <td>-3.694943e-02</td>\n      <td>-8.660434e-03</td>\n      <td>...</td>\n      <td>4.473573e-02</td>\n      <td>1.440591e-01</td>\n      <td>5.114236e-02</td>\n      <td>-1.618187e-02</td>\n      <td>-2.330828e-01</td>\n      <td>-4.140710e-02</td>\n      <td>-5.134591e-03</td>\n      <td>-9.412688e-03</td>\n      <td>-0.010596</td>\n      <td>-0.012323</td>\n    </tr>\n    <tr>\n      <th>V1</th>\n      <td>0.117396</td>\n      <td>1.000000e+00</td>\n      <td>4.697350e-17</td>\n      <td>-1.424390e-15</td>\n      <td>1.755316e-17</td>\n      <td>6.391162e-17</td>\n      <td>2.398071e-16</td>\n      <td>1.991550e-15</td>\n      <td>-9.490675e-17</td>\n      <td>2.169581e-16</td>\n      <td>...</td>\n      <td>-1.755072e-16</td>\n      <td>7.477367e-17</td>\n      <td>9.808705e-16</td>\n      <td>7.354269e-17</td>\n      <td>-9.805358e-16</td>\n      <td>-8.621897e-17</td>\n      <td>3.208233e-17</td>\n      <td>9.820892e-16</td>\n      <td>-0.227709</td>\n      <td>-0.101347</td>\n    </tr>\n    <tr>\n      <th>V2</th>\n      <td>-0.010593</td>\n      <td>4.697350e-17</td>\n      <td>1.000000e+00</td>\n      <td>2.512175e-16</td>\n      <td>-1.126388e-16</td>\n      <td>-2.039868e-16</td>\n      <td>5.024680e-16</td>\n      <td>3.966486e-16</td>\n      <td>-4.413984e-17</td>\n      <td>-5.728718e-17</td>\n      <td>...</td>\n      <td>8.444409e-17</td>\n      <td>2.500830e-16</td>\n      <td>1.059562e-16</td>\n      <td>-8.142354e-18</td>\n      <td>-4.261894e-17</td>\n      <td>2.601622e-16</td>\n      <td>-4.478472e-16</td>\n      <td>-3.676415e-16</td>\n      <td>-0.531409</td>\n      <td>0.091289</td>\n    </tr>\n    <tr>\n      <th>V3</th>\n      <td>-0.419618</td>\n      <td>-1.424390e-15</td>\n      <td>2.512175e-16</td>\n      <td>1.000000e+00</td>\n      <td>-3.416910e-16</td>\n      <td>-1.436514e-15</td>\n      <td>1.431581e-15</td>\n      <td>2.168574e-15</td>\n      <td>3.433113e-16</td>\n      <td>-4.233770e-16</td>\n      <td>...</td>\n      <td>-2.971969e-17</td>\n      <td>4.648259e-16</td>\n      <td>2.115206e-17</td>\n      <td>-9.351637e-17</td>\n      <td>4.771164e-16</td>\n      <td>6.521501e-16</td>\n      <td>6.239832e-16</td>\n      <td>7.726948e-16</td>\n      <td>-0.210880</td>\n      <td>-0.192961</td>\n    </tr>\n    <tr>\n      <th>V4</th>\n      <td>-0.105260</td>\n      <td>1.755316e-17</td>\n      <td>-1.126388e-16</td>\n      <td>-3.416910e-16</td>\n      <td>1.000000e+00</td>\n      <td>-1.940929e-15</td>\n      <td>-2.712659e-16</td>\n      <td>1.556330e-16</td>\n      <td>5.195643e-16</td>\n      <td>3.859585e-16</td>\n      <td>...</td>\n      <td>-9.976950e-17</td>\n      <td>2.099922e-16</td>\n      <td>6.002528e-17</td>\n      <td>2.229738e-16</td>\n      <td>5.394585e-16</td>\n      <td>-6.179751e-16</td>\n      <td>-6.403423e-17</td>\n      <td>-5.863664e-17</td>\n      <td>0.098732</td>\n      <td>0.133447</td>\n    </tr>\n    <tr>\n      <th>V5</th>\n      <td>0.173072</td>\n      <td>6.391162e-17</td>\n      <td>-2.039868e-16</td>\n      <td>-1.436514e-15</td>\n      <td>-1.940929e-15</td>\n      <td>1.000000e+00</td>\n      <td>7.926364e-16</td>\n      <td>-4.209851e-16</td>\n      <td>7.589187e-16</td>\n      <td>4.205206e-16</td>\n      <td>...</td>\n      <td>-1.368701e-16</td>\n      <td>5.060029e-16</td>\n      <td>1.637596e-16</td>\n      <td>-9.286095e-16</td>\n      <td>5.625102e-16</td>\n      <td>9.144690e-16</td>\n      <td>4.465960e-16</td>\n      <td>-3.299167e-16</td>\n      <td>-0.386356</td>\n      <td>-0.094974</td>\n    </tr>\n    <tr>\n      <th>V6</th>\n      <td>-0.063016</td>\n      <td>2.398071e-16</td>\n      <td>5.024680e-16</td>\n      <td>1.431581e-15</td>\n      <td>-2.712659e-16</td>\n      <td>7.926364e-16</td>\n      <td>1.000000e+00</td>\n      <td>1.429426e-16</td>\n      <td>-1.707421e-16</td>\n      <td>1.114447e-16</td>\n      <td>...</td>\n      <td>-1.575903e-16</td>\n      <td>-3.362902e-16</td>\n      <td>-7.232186e-17</td>\n      <td>-1.261867e-15</td>\n      <td>1.081933e-15</td>\n      <td>-2.378414e-16</td>\n      <td>-2.623818e-16</td>\n      <td>4.813155e-16</td>\n      <td>0.215981</td>\n      <td>-0.043643</td>\n    </tr>\n    <tr>\n      <th>V7</th>\n      <td>0.084714</td>\n      <td>1.991550e-15</td>\n      <td>3.966486e-16</td>\n      <td>2.168574e-15</td>\n      <td>1.556330e-16</td>\n      <td>-4.209851e-16</td>\n      <td>1.429426e-16</td>\n      <td>1.000000e+00</td>\n      <td>-8.691834e-17</td>\n      <td>7.933251e-16</td>\n      <td>...</td>\n      <td>1.938604e-16</td>\n      <td>-1.058131e-15</td>\n      <td>2.327911e-16</td>\n      <td>-2.589727e-17</td>\n      <td>1.174169e-15</td>\n      <td>-7.334507e-16</td>\n      <td>-5.886825e-16</td>\n      <td>-6.836764e-17</td>\n      <td>0.397311</td>\n      <td>-0.187257</td>\n    </tr>\n    <tr>\n      <th>V8</th>\n      <td>-0.036949</td>\n      <td>-9.490675e-17</td>\n      <td>-4.413984e-17</td>\n      <td>3.433113e-16</td>\n      <td>5.195643e-16</td>\n      <td>7.589187e-16</td>\n      <td>-1.707421e-16</td>\n      <td>-8.691834e-17</td>\n      <td>1.000000e+00</td>\n      <td>2.900829e-16</td>\n      <td>...</td>\n      <td>-2.412439e-16</td>\n      <td>5.475559e-16</td>\n      <td>3.897104e-16</td>\n      <td>-1.802967e-16</td>\n      <td>-1.390791e-16</td>\n      <td>-1.209975e-16</td>\n      <td>1.733633e-16</td>\n      <td>-4.484325e-16</td>\n      <td>-0.103079</td>\n      <td>0.019875</td>\n    </tr>\n    <tr>\n      <th>V9</th>\n      <td>-0.008660</td>\n      <td>2.169581e-16</td>\n      <td>-5.728718e-17</td>\n      <td>-4.233770e-16</td>\n      <td>3.859585e-16</td>\n      <td>4.205206e-16</td>\n      <td>1.114447e-16</td>\n      <td>7.933251e-16</td>\n      <td>2.900829e-16</td>\n      <td>1.000000e+00</td>\n      <td>...</td>\n      <td>4.578389e-17</td>\n      <td>2.871855e-17</td>\n      <td>5.929286e-16</td>\n      <td>-2.346385e-16</td>\n      <td>1.099645e-15</td>\n      <td>-1.388725e-15</td>\n      <td>-2.287414e-16</td>\n      <td>9.146779e-16</td>\n      <td>-0.044246</td>\n      <td>-0.097733</td>\n    </tr>\n    <tr>\n      <th>V10</th>\n      <td>0.030617</td>\n      <td>7.433820e-17</td>\n      <td>-4.782388e-16</td>\n      <td>6.289267e-16</td>\n      <td>6.055490e-16</td>\n      <td>-6.601716e-16</td>\n      <td>2.850776e-16</td>\n      <td>3.043333e-17</td>\n      <td>9.051847e-17</td>\n      <td>-2.771761e-16</td>\n      <td>...</td>\n      <td>8.089504e-16</td>\n      <td>-6.707598e-16</td>\n      <td>3.809732e-16</td>\n      <td>-4.032806e-17</td>\n      <td>-2.863813e-16</td>\n      <td>-2.554293e-16</td>\n      <td>-3.103239e-16</td>\n      <td>-1.515934e-16</td>\n      <td>-0.101502</td>\n      <td>-0.216883</td>\n    </tr>\n    <tr>\n      <th>V11</th>\n      <td>-0.247689</td>\n      <td>2.438580e-16</td>\n      <td>9.468995e-16</td>\n      <td>-5.501758e-17</td>\n      <td>-2.083600e-16</td>\n      <td>7.342759e-16</td>\n      <td>4.865799e-16</td>\n      <td>-1.084105e-15</td>\n      <td>1.954747e-16</td>\n      <td>4.682341e-16</td>\n      <td>...</td>\n      <td>-3.911893e-16</td>\n      <td>-3.811640e-17</td>\n      <td>2.232007e-16</td>\n      <td>1.219849e-15</td>\n      <td>-4.567635e-16</td>\n      <td>-1.110976e-16</td>\n      <td>-2.635827e-16</td>\n      <td>-3.091914e-16</td>\n      <td>0.000104</td>\n      <td>0.154876</td>\n    </tr>\n    <tr>\n      <th>V12</th>\n      <td>0.124348</td>\n      <td>2.422086e-16</td>\n      <td>-6.588252e-16</td>\n      <td>2.206522e-16</td>\n      <td>-5.657963e-16</td>\n      <td>3.761033e-16</td>\n      <td>2.140589e-16</td>\n      <td>1.510045e-15</td>\n      <td>-6.266057e-17</td>\n      <td>-2.445230e-15</td>\n      <td>...</td>\n      <td>3.229576e-16</td>\n      <td>-5.903992e-16</td>\n      <td>1.392162e-16</td>\n      <td>4.901644e-16</td>\n      <td>5.053736e-16</td>\n      <td>-5.759321e-16</td>\n      <td>-2.312619e-16</td>\n      <td>7.327446e-16</td>\n      <td>-0.009542</td>\n      <td>-0.260593</td>\n    </tr>\n    <tr>\n      <th>V13</th>\n      <td>-0.065902</td>\n      <td>-2.115458e-16</td>\n      <td>3.854521e-16</td>\n      <td>-6.883375e-16</td>\n      <td>-1.506129e-16</td>\n      <td>-9.578659e-16</td>\n      <td>-2.268061e-16</td>\n      <td>-9.892325e-17</td>\n      <td>-2.382948e-16</td>\n      <td>-2.650351e-16</td>\n      <td>...</td>\n      <td>9.499130e-17</td>\n      <td>-2.659710e-17</td>\n      <td>-5.884304e-16</td>\n      <td>-5.470547e-16</td>\n      <td>8.066738e-17</td>\n      <td>-2.121518e-16</td>\n      <td>-4.520414e-16</td>\n      <td>1.049541e-15</td>\n      <td>0.005293</td>\n      <td>-0.004570</td>\n    </tr>\n    <tr>\n      <th>V14</th>\n      <td>-0.098757</td>\n      <td>9.352582e-16</td>\n      <td>-2.541036e-16</td>\n      <td>4.271336e-16</td>\n      <td>-8.522435e-17</td>\n      <td>-3.634803e-16</td>\n      <td>3.452801e-16</td>\n      <td>-1.729462e-16</td>\n      <td>-1.131098e-16</td>\n      <td>2.343317e-16</td>\n      <td>...</td>\n      <td>1.634141e-17</td>\n      <td>3.439699e-16</td>\n      <td>7.620728e-17</td>\n      <td>2.335749e-16</td>\n      <td>-2.606783e-16</td>\n      <td>-6.580254e-18</td>\n      <td>1.285770e-16</td>\n      <td>2.503271e-15</td>\n      <td>0.033751</td>\n      <td>-0.302544</td>\n    </tr>\n    <tr>\n      <th>V15</th>\n      <td>-0.183453</td>\n      <td>-3.252451e-16</td>\n      <td>2.831060e-16</td>\n      <td>1.122756e-16</td>\n      <td>-1.507718e-16</td>\n      <td>-5.132620e-16</td>\n      <td>-6.368111e-18</td>\n      <td>1.936832e-17</td>\n      <td>2.021491e-16</td>\n      <td>-1.588105e-15</td>\n      <td>...</td>\n      <td>1.947458e-17</td>\n      <td>-8.936817e-16</td>\n      <td>1.119827e-16</td>\n      <td>-4.589689e-16</td>\n      <td>3.869740e-16</td>\n      <td>3.761094e-16</td>\n      <td>-1.265235e-15</td>\n      <td>-1.063286e-15</td>\n      <td>-0.002986</td>\n      <td>-0.004223</td>\n    </tr>\n    <tr>\n      <th>V16</th>\n      <td>0.011903</td>\n      <td>6.308789e-16</td>\n      <td>4.934097e-17</td>\n      <td>1.183364e-15</td>\n      <td>-6.939204e-16</td>\n      <td>-3.517076e-16</td>\n      <td>-2.477917e-16</td>\n      <td>2.893672e-16</td>\n      <td>5.027192e-16</td>\n      <td>-3.251906e-16</td>\n      <td>...</td>\n      <td>-3.927401e-16</td>\n      <td>3.878384e-17</td>\n      <td>8.519670e-16</td>\n      <td>-4.289239e-16</td>\n      <td>-6.644104e-16</td>\n      <td>-5.186503e-16</td>\n      <td>7.820038e-16</td>\n      <td>8.637186e-16</td>\n      <td>-0.003910</td>\n      <td>-0.196539</td>\n    </tr>\n    <tr>\n      <th>V17</th>\n      <td>-0.073297</td>\n      <td>-5.011524e-16</td>\n      <td>-9.883008e-16</td>\n      <td>4.576619e-17</td>\n      <td>-4.397925e-16</td>\n      <td>1.425729e-16</td>\n      <td>3.567582e-16</td>\n      <td>1.149692e-15</td>\n      <td>-3.508777e-16</td>\n      <td>6.535992e-16</td>\n      <td>...</td>\n      <td>-7.753967e-16</td>\n      <td>-8.389703e-16</td>\n      <td>5.367784e-16</td>\n      <td>-5.543631e-17</td>\n      <td>4.822068e-16</td>\n      <td>4.870302e-16</td>\n      <td>8.844373e-16</td>\n      <td>-2.182692e-16</td>\n      <td>0.007309</td>\n      <td>-0.326481</td>\n    </tr>\n    <tr>\n      <th>V18</th>\n      <td>0.090438</td>\n      <td>2.870125e-16</td>\n      <td>2.636654e-16</td>\n      <td>5.427965e-16</td>\n      <td>1.493667e-16</td>\n      <td>1.109525e-15</td>\n      <td>2.811474e-16</td>\n      <td>-1.116789e-16</td>\n      <td>-4.093852e-16</td>\n      <td>1.203843e-16</td>\n      <td>...</td>\n      <td>-1.140973e-15</td>\n      <td>-8.662635e-17</td>\n      <td>-3.624236e-16</td>\n      <td>-1.126043e-16</td>\n      <td>-2.310856e-16</td>\n      <td>3.183964e-16</td>\n      <td>2.435170e-16</td>\n      <td>8.844995e-16</td>\n      <td>0.035650</td>\n      <td>-0.111485</td>\n    </tr>\n    <tr>\n      <th>V19</th>\n      <td>0.028975</td>\n      <td>1.818128e-16</td>\n      <td>9.528280e-17</td>\n      <td>2.576773e-16</td>\n      <td>-2.656938e-16</td>\n      <td>-3.138234e-16</td>\n      <td>2.717167e-16</td>\n      <td>-2.874017e-16</td>\n      <td>-5.339821e-16</td>\n      <td>1.120752e-16</td>\n      <td>...</td>\n      <td>4.032541e-16</td>\n      <td>-9.690436e-16</td>\n      <td>5.733798e-16</td>\n      <td>3.126716e-17</td>\n      <td>7.415355e-16</td>\n      <td>5.614354e-16</td>\n      <td>-1.113035e-16</td>\n      <td>-1.375843e-15</td>\n      <td>-0.056151</td>\n      <td>0.034783</td>\n    </tr>\n    <tr>\n      <th>V20</th>\n      <td>-0.050866</td>\n      <td>1.036959e-16</td>\n      <td>-9.309954e-16</td>\n      <td>-9.429297e-16</td>\n      <td>-3.223123e-16</td>\n      <td>2.076048e-16</td>\n      <td>1.898638e-16</td>\n      <td>1.744242e-16</td>\n      <td>-1.095534e-16</td>\n      <td>-4.340941e-16</td>\n      <td>...</td>\n      <td>-1.120828e-15</td>\n      <td>1.105842e-15</td>\n      <td>4.986739e-16</td>\n      <td>1.637488e-16</td>\n      <td>-1.518242e-16</td>\n      <td>-2.975081e-16</td>\n      <td>-1.446069e-15</td>\n      <td>-1.133579e-16</td>\n      <td>0.339403</td>\n      <td>0.020090</td>\n    </tr>\n    <tr>\n      <th>V21</th>\n      <td>0.044736</td>\n      <td>-1.755072e-16</td>\n      <td>8.444409e-17</td>\n      <td>-2.971969e-17</td>\n      <td>-9.976950e-17</td>\n      <td>-1.368701e-16</td>\n      <td>-1.575903e-16</td>\n      <td>1.938604e-16</td>\n      <td>-2.412439e-16</td>\n      <td>4.578389e-17</td>\n      <td>...</td>\n      <td>1.000000e+00</td>\n      <td>3.905948e-15</td>\n      <td>6.127323e-16</td>\n      <td>1.298254e-16</td>\n      <td>-2.826293e-16</td>\n      <td>-4.907301e-16</td>\n      <td>-1.033403e-15</td>\n      <td>5.132234e-16</td>\n      <td>0.105999</td>\n      <td>0.040413</td>\n    </tr>\n    <tr>\n      <th>V22</th>\n      <td>0.144059</td>\n      <td>7.477367e-17</td>\n      <td>2.500830e-16</td>\n      <td>4.648259e-16</td>\n      <td>2.099922e-16</td>\n      <td>5.060029e-16</td>\n      <td>-3.362902e-16</td>\n      <td>-1.058131e-15</td>\n      <td>5.475559e-16</td>\n      <td>2.871855e-17</td>\n      <td>...</td>\n      <td>3.905948e-15</td>\n      <td>1.000000e+00</td>\n      <td>3.130812e-16</td>\n      <td>1.150829e-17</td>\n      <td>-6.078986e-16</td>\n      <td>-8.477050e-16</td>\n      <td>-1.294910e-16</td>\n      <td>-3.021376e-16</td>\n      <td>-0.064801</td>\n      <td>0.000805</td>\n    </tr>\n    <tr>\n      <th>V23</th>\n      <td>0.051142</td>\n      <td>9.808705e-16</td>\n      <td>1.059562e-16</td>\n      <td>2.115206e-17</td>\n      <td>6.002528e-17</td>\n      <td>1.637596e-16</td>\n      <td>-7.232186e-17</td>\n      <td>2.327911e-16</td>\n      <td>3.897104e-16</td>\n      <td>5.929286e-16</td>\n      <td>...</td>\n      <td>6.127323e-16</td>\n      <td>3.130812e-16</td>\n      <td>1.000000e+00</td>\n      <td>-4.411271e-17</td>\n      <td>-9.938362e-16</td>\n      <td>8.848700e-16</td>\n      <td>5.524044e-16</td>\n      <td>9.029821e-16</td>\n      <td>-0.112633</td>\n      <td>-0.002685</td>\n    </tr>\n    <tr>\n      <th>V24</th>\n      <td>-0.016182</td>\n      <td>7.354269e-17</td>\n      <td>-8.142354e-18</td>\n      <td>-9.351637e-17</td>\n      <td>2.229738e-16</td>\n      <td>-9.286095e-16</td>\n      <td>-1.261867e-15</td>\n      <td>-2.589727e-17</td>\n      <td>-1.802967e-16</td>\n      <td>-2.346385e-16</td>\n      <td>...</td>\n      <td>1.298254e-16</td>\n      <td>1.150829e-17</td>\n      <td>-4.411271e-17</td>\n      <td>1.000000e+00</td>\n      <td>1.557318e-15</td>\n      <td>3.129195e-16</td>\n      <td>-3.736529e-16</td>\n      <td>-2.259275e-16</td>\n      <td>0.005146</td>\n      <td>-0.007221</td>\n    </tr>\n    <tr>\n      <th>V25</th>\n      <td>-0.233083</td>\n      <td>-9.805358e-16</td>\n      <td>-4.261894e-17</td>\n      <td>4.771164e-16</td>\n      <td>5.394585e-16</td>\n      <td>5.625102e-16</td>\n      <td>1.081933e-15</td>\n      <td>1.174169e-15</td>\n      <td>-1.390791e-16</td>\n      <td>1.099645e-15</td>\n      <td>...</td>\n      <td>-2.826293e-16</td>\n      <td>-6.078986e-16</td>\n      <td>-9.938362e-16</td>\n      <td>1.557318e-15</td>\n      <td>1.000000e+00</td>\n      <td>2.810884e-15</td>\n      <td>-6.107118e-16</td>\n      <td>3.399375e-16</td>\n      <td>-0.047837</td>\n      <td>0.003308</td>\n    </tr>\n    <tr>\n      <th>V26</th>\n      <td>-0.041407</td>\n      <td>-8.621897e-17</td>\n      <td>2.601622e-16</td>\n      <td>6.521501e-16</td>\n      <td>-6.179751e-16</td>\n      <td>9.144690e-16</td>\n      <td>-2.378414e-16</td>\n      <td>-7.334507e-16</td>\n      <td>-1.209975e-16</td>\n      <td>-1.388725e-15</td>\n      <td>...</td>\n      <td>-4.907301e-16</td>\n      <td>-8.477050e-16</td>\n      <td>8.848700e-16</td>\n      <td>3.129195e-16</td>\n      <td>2.810884e-15</td>\n      <td>1.000000e+00</td>\n      <td>-3.383861e-16</td>\n      <td>-3.751403e-16</td>\n      <td>-0.003208</td>\n      <td>0.004455</td>\n    </tr>\n    <tr>\n      <th>V27</th>\n      <td>-0.005135</td>\n      <td>3.208233e-17</td>\n      <td>-4.478472e-16</td>\n      <td>6.239832e-16</td>\n      <td>-6.403423e-17</td>\n      <td>4.465960e-16</td>\n      <td>-2.623818e-16</td>\n      <td>-5.886825e-16</td>\n      <td>1.733633e-16</td>\n      <td>-2.287414e-16</td>\n      <td>...</td>\n      <td>-1.033403e-15</td>\n      <td>-1.294910e-16</td>\n      <td>5.524044e-16</td>\n      <td>-3.736529e-16</td>\n      <td>-6.107118e-16</td>\n      <td>-3.383861e-16</td>\n      <td>1.000000e+00</td>\n      <td>-3.770124e-16</td>\n      <td>0.028825</td>\n      <td>0.017580</td>\n    </tr>\n    <tr>\n      <th>V28</th>\n      <td>-0.009413</td>\n      <td>9.820892e-16</td>\n      <td>-3.676415e-16</td>\n      <td>7.726948e-16</td>\n      <td>-5.863664e-17</td>\n      <td>-3.299167e-16</td>\n      <td>4.813155e-16</td>\n      <td>-6.836764e-17</td>\n      <td>-4.484325e-16</td>\n      <td>9.146779e-16</td>\n      <td>...</td>\n      <td>5.132234e-16</td>\n      <td>-3.021376e-16</td>\n      <td>9.029821e-16</td>\n      <td>-2.259275e-16</td>\n      <td>3.399375e-16</td>\n      <td>-3.751403e-16</td>\n      <td>-3.770124e-16</td>\n      <td>1.000000e+00</td>\n      <td>0.010258</td>\n      <td>0.009536</td>\n    </tr>\n    <tr>\n      <th>Amount</th>\n      <td>-0.010596</td>\n      <td>-2.277087e-01</td>\n      <td>-5.314089e-01</td>\n      <td>-2.108805e-01</td>\n      <td>9.873167e-02</td>\n      <td>-3.863563e-01</td>\n      <td>2.159812e-01</td>\n      <td>3.973113e-01</td>\n      <td>-1.030791e-01</td>\n      <td>-4.424560e-02</td>\n      <td>...</td>\n      <td>1.059989e-01</td>\n      <td>-6.480065e-02</td>\n      <td>-1.126326e-01</td>\n      <td>5.146217e-03</td>\n      <td>-4.783686e-02</td>\n      <td>-3.208037e-03</td>\n      <td>2.882546e-02</td>\n      <td>1.025822e-02</td>\n      <td>1.000000</td>\n      <td>0.005632</td>\n    </tr>\n    <tr>\n      <th>Class</th>\n      <td>-0.012323</td>\n      <td>-1.013473e-01</td>\n      <td>9.128865e-02</td>\n      <td>-1.929608e-01</td>\n      <td>1.334475e-01</td>\n      <td>-9.497430e-02</td>\n      <td>-4.364316e-02</td>\n      <td>-1.872566e-01</td>\n      <td>1.987512e-02</td>\n      <td>-9.773269e-02</td>\n      <td>...</td>\n      <td>4.041338e-02</td>\n      <td>8.053175e-04</td>\n      <td>-2.685156e-03</td>\n      <td>-7.220907e-03</td>\n      <td>3.307706e-03</td>\n      <td>4.455398e-03</td>\n      <td>1.757973e-02</td>\n      <td>9.536041e-03</td>\n      <td>0.005632</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>31 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time 0\nV1 0\nV2 0\nV3 0\nV4 0\nV5 0\nV6 0\nV7 0\nV8 0\nV9 0\nV10 0\nV11 0\nV12 0\nV13 0\nV14 0\nV15 0\nV16 0\nV17 0\nV18 0\nV19 0\nV20 0\nV21 0\nV22 0\nV23 0\nV24 0\nV25 0\nV26 0\nV27 0\nV28 0\nAmount 0\nClass 0\n"
     ]
    }
   ],
   "source": [
    "for s in data.columns:\n",
    "    print(str(s) + \" \" + str(data[s].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "data['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['Class']\n",
    "features = data.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(284807,)"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[[93832     6]\n",
      " [   35   114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93838\n",
      "           1       0.95      0.77      0.85       149\n",
      "\n",
      "    accuracy                           1.00     93987\n",
      "   macro avg       0.97      0.88      0.92     93987\n",
      "weighted avg       1.00      1.00      1.00     93987\n",
      "\n",
      "0.9995637694574782\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "Model ROC value\n",
      "0.9392821213655234\n",
      "Training ROC value\n",
      "0.9999994566349624\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "rf_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = rf_model.predict(X_train)\n",
    "train_rf_probs = rf_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[[93833     5]\n",
      " [   38   111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93838\n",
      "           1       0.96      0.74      0.84       149\n",
      "\n",
      "    accuracy                           1.00     93987\n",
      "   macro avg       0.98      0.87      0.92     93987\n",
      "weighted avg       1.00      1.00      1.00     93987\n",
      "\n",
      "0.9995424899188186\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Model ROC value\n",
      "0.9393346537106431\n",
      "Training ROC value\n",
      "0.9999995867364502\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "bal_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   class_weight=\"balanced\",\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "bal_model.fit(X_train, y_train)\n",
    "y_pred = bal_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = bal_model.predict(X_train)\n",
    "train_rf_probs = bal_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = bal_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = bal_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[[93834     4]\n",
      " [   37   112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93838\n",
      "           1       0.97      0.75      0.85       149\n",
      "\n",
      "    accuracy                           1.00     93987\n",
      "   macro avg       0.98      0.88      0.92     93987\n",
      "weighted avg       1.00      1.00      1.00     93987\n",
      "\n",
      "0.9995637694574782\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Model ROC value\n",
      "0.9426662915139629\n",
      "Training ROC value\n",
      "0.9999997244909669\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "bal_sub_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   class_weight=\"balanced_subsample\",\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "bal_sub_model.fit(X_train, y_train)\n",
    "y_pred = bal_sub_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = bal_sub_model.predict(X_train)\n",
    "train_rf_probs = bal_sub_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = bal_sub_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = bal_sub_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541</th>\n      <td>406.0</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>...</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>472.0</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4920</th>\n      <td>4462.0</td>\n      <td>-2.303350</td>\n      <td>1.759247</td>\n      <td>-0.359745</td>\n      <td>2.330243</td>\n      <td>-0.821628</td>\n      <td>-0.075788</td>\n      <td>0.562320</td>\n      <td>-0.399147</td>\n      <td>-0.238253</td>\n      <td>...</td>\n      <td>-0.294166</td>\n      <td>-0.932391</td>\n      <td>0.172726</td>\n      <td>-0.087330</td>\n      <td>-0.156114</td>\n      <td>-0.542628</td>\n      <td>0.039566</td>\n      <td>-0.153029</td>\n      <td>239.93</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6108</th>\n      <td>6986.0</td>\n      <td>-4.397974</td>\n      <td>1.358367</td>\n      <td>-2.592844</td>\n      <td>2.679787</td>\n      <td>-1.128131</td>\n      <td>-1.706536</td>\n      <td>-3.496197</td>\n      <td>-0.248778</td>\n      <td>-0.247768</td>\n      <td>...</td>\n      <td>0.573574</td>\n      <td>0.176968</td>\n      <td>-0.436207</td>\n      <td>-0.053502</td>\n      <td>0.252405</td>\n      <td>-0.657488</td>\n      <td>-0.827136</td>\n      <td>0.849573</td>\n      <td>59.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6329</th>\n      <td>7519.0</td>\n      <td>1.234235</td>\n      <td>3.019740</td>\n      <td>-4.304597</td>\n      <td>4.732795</td>\n      <td>3.624201</td>\n      <td>-1.357746</td>\n      <td>1.713445</td>\n      <td>-0.496358</td>\n      <td>-1.282858</td>\n      <td>...</td>\n      <td>-0.379068</td>\n      <td>-0.704181</td>\n      <td>-0.656805</td>\n      <td>-1.632653</td>\n      <td>1.488901</td>\n      <td>0.566797</td>\n      <td>-0.010016</td>\n      <td>0.146793</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "positive_cases = data[data['Class'] == 1]\n",
    "positive_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 550):\n",
    "    data = data.append(positive_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    284315\n",
       "1    271092\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_bal = data['Class']\n",
    "features_bal = data.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_bal, labels_bal, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[[93850     7]\n",
      " [    0 89428]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93857\n",
      "           1       1.00      1.00      1.00     89428\n",
      "\n",
      "    accuracy                           1.00    183285\n",
      "   macro avg       1.00      1.00      1.00    183285\n",
      "weighted avg       1.00      1.00      1.00    183285\n",
      "\n",
      "0.999961808113048\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "Model ROC value\n",
      "0.9999893454936766\n",
      "Training ROC value\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "rf_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = rf_model.predict(X_train)\n",
    "train_rf_probs = rf_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[[93850     7]\n",
      " [    0 89428]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93857\n",
      "           1       1.00      1.00      1.00     89428\n",
      "\n",
      "    accuracy                           1.00    183285\n",
      "   macro avg       1.00      1.00      1.00    183285\n",
      "weighted avg       1.00      1.00      1.00    183285\n",
      "\n",
      "0.999961808113048\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "Model ROC value\n",
      "0.9999840182405149\n",
      "Training ROC value\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "bal_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   class_weight=\"balanced\",\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "bal_model.fit(X_train, y_train)\n",
    "y_pred = bal_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = bal_model.predict(X_train)\n",
    "train_rf_probs = bal_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = bal_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = bal_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[[93849     8]\n",
      " [    0 89428]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93857\n",
      "           1       1.00      1.00      1.00     89428\n",
      "\n",
      "    accuracy                           1.00    183285\n",
      "   macro avg       1.00      1.00      1.00    183285\n",
      "weighted avg       1.00      1.00      1.00    183285\n",
      "\n",
      "0.9999563521291976\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "Model ROC value\n",
      "0.999978690987353\n",
      "Training ROC value\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 100 trees\n",
    "bal_sub_model = RandomForestClassifier(n_estimators=10,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=50,\n",
    "                                   class_weight=\"balanced_subsample\",\n",
    "                                   verbose=True)\n",
    "# Fit on training data\n",
    "bal_sub_model.fit(X_train, y_train)\n",
    "y_pred = bal_sub_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "train_rf_predictions = bal_sub_model.predict(X_train)\n",
    "train_rf_probs = bal_sub_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = bal_sub_model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = bal_sub_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(y_test, rf_probs)\n",
    "roc_value_train = roc_auc_score(y_train, train_rf_probs)\n",
    "\n",
    "print(\"Model ROC value\")\n",
    "print(roc_value)\n",
    "\n",
    "print(\"Training ROC value\")\n",
    "print(roc_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(bal_sub_model.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',                                                                 ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        importance\nV14       0.328061\nV11       0.128315\nV10       0.090616\nV3        0.061711\nV4        0.046239\nV17       0.042934\nV16       0.040907\nV21       0.039143\nV12       0.034887\nV6        0.031314\nV9        0.015531\nAmount    0.012053\nV19       0.011396\nV7        0.009533\nV20       0.008850\nV2        0.008462\nV25       0.008093\nV23       0.007924\nV13       0.007656\nV26       0.007523\nV1        0.007309\nV5        0.006795\nV15       0.006663\nV27       0.006470\nV24       0.006398\nV8        0.006153\nV28       0.005408\nV22       0.004897\nV18       0.004549\nTime      0.004206\n"
     ]
    }
   ],
   "source": [
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}